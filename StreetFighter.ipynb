{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "from gym import Env\n",
    "import numpy as np\n",
    "import gym\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SF2Discretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env=env, combos=[[],['DOWN'],['DOWN','LEFT'],['DOWN','RIGHT'],['LEFT'],['RIGHT'],['UP'],['UP','LEFT'],['UP','RIGHT'],['A'],['B'],['C'],['X'],['Y'],['Z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighterEnv(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.game = SF2Discretizer(retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions=retro.Actions.FILTERED))\n",
    "        self.observation_space = self.game.observation_space\n",
    "        self.action_space = self.game.action_space\n",
    "        \n",
    "        # Create a dedicated random number generator for the environment\n",
    "        self.np_random = np.random.RandomState()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.game.step(action)\n",
    "\n",
    "        # Custom hp-based reward to encourage defense and attack\n",
    "        if self.previous_enemy_health is None or self.previous_health is None:\n",
    "            reward = 0\n",
    "        else:\n",
    "            # Reward is the hp changes\n",
    "            reward = (self.previous_enemy_health - info['enemy_health']) - (self.previous_health - info['health'])\n",
    "        self.previous_enemy_health = info['enemy_health']\n",
    "        self.previous_health = info['health']\n",
    "\n",
    "        # Stop on life loss\n",
    "        if info['enemy_health'] <= 0 or info['health'] <= 0:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.game.reset()\n",
    "        self.previous_enemy_health = None\n",
    "        self.previous_health = None\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode=\"human\"): \n",
    "        return self.game.render(mode=mode)\n",
    "    \n",
    "    def close(self): \n",
    "        self.game.close()\n",
    "\n",
    "    # The functions betlow is required for AtariWrapper\n",
    "\n",
    "    def get_action_meaning(self, act):\n",
    "        return 'NOOP' if act == 0 else self.game.get_action_meaning(self.game.action(act))\n",
    "    \n",
    "    def get_action_meanings(self):\n",
    "        return [self.get_action_meaning(act) for act in range(self.action_space.n)]\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        # if there is no seed, return an empty list\n",
    "        if seed is None:\n",
    "            return []\n",
    "        # set the random number seed for the NumPy random number generator\n",
    "        self.np_random.seed(seed)\n",
    "        # return the list of seeds used by RNG(s) in the environment\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecFrameStack, VecMonitor, DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.utils import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(rank: int, seed: int = 0):\n",
    "    def _init():\n",
    "        # Create the base environment\n",
    "        env = StreetFighterEnv()\n",
    "        env.seed(seed + rank)\n",
    "\n",
    "        # Random NOOP at the fight start\n",
    "        # Accumulate rewards every 4 frames\n",
    "        # Reduce resolution to 84 pixels grayscale\n",
    "        env = AtariWrapper(env, noop_max=10, frame_skip=4, screen_size=84, terminal_on_life_loss=False, clip_reward=False)\n",
    "\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "def make_vec_env(num_cpu: int):\n",
    "    # Create the vectorized environment\n",
    "    if num_cpu > 1:\n",
    "        env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])\n",
    "    else:\n",
    "        env = DummyVecEnv([make_env(i) for i in range(num_cpu)])\n",
    "\n",
    "    # Stacking frames to detect movement\n",
    "    env = VecFrameStack(env, 4, channels_order=\"last\")\n",
    "\n",
    "    # Monitor episode reward and length\n",
    "    env = VecMonitor(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model saving callback\n",
    "callback = CheckpointCallback(save_freq=10000, save_path=CHECKPOINT_DIR, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_params = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'batch_size': 128,\n",
    "    'gamma': 0.9, # Discount factor for future reward, reduced from 0.99 because the agent only need to see a few step ahead\n",
    "    'n_steps': 1024, # Steps to run per update, enough to pass an episode\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of processes to use\n",
    "num_cpu = 8\n",
    "print(f'Loading {num_cpu} environments for training')\n",
    "env = make_vec_env(num_cpu)\n",
    "\n",
    "# This is the AI model started\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, **ppo_params)\n",
    "# model = PPO.load('./checkpoints/rl_model_13050624_steps', env, verbose=1, tensorboard_log=LOG_DIR, **ppo_params)\n",
    "\n",
    "# Train the AI model, this is where the AI model starts to learn\n",
    "try:\n",
    "    model.learn(total_timesteps=1e7, callback=callback, reset_num_timesteps=False)\n",
    "finally:\n",
    "    env.close()\n",
    "    model.save(CHECKPOINT_DIR + 'latest_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test It Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = PPO.load('./latest_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test environment\n",
    "env = make_vec_env(1)\n",
    "\n",
    "# Loop through the games\n",
    "games = 5\n",
    "for i in range(games):\n",
    "    # Reset game to starting state\n",
    "    images = []\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "    \n",
    "    # Make a GIF for each game\n",
    "    imageio.mimsave(f'./replays/game_{i + 1}.gif', images, fps=24)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2575392019334285e0602a4035eec46b9260ee4c95297ea34ade6e3c8b8fcaf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
